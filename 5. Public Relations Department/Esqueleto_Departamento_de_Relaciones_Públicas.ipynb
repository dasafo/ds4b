{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA # 1: COMPRENDER EL PROBLEMA Y EL CASO PRÁCTICO\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://drive.google.com/uc?id=11BquVVgQTebvVO5NZ2TGA526rulbWBv5\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figura 1. Análisis del Sentimiento de los Clientes\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1nCRtae4P_PA29o7_ZZxT4KZ_Dwn9pVto)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1KWse1Fc_seaqEfgnKMTdFRM1Xmo_ZFzb)\n",
    "\n",
    "# TAREA # 2: IMPORTAR LIBRERÍAS Y DATA SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandasgui import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitarás montar tu disco usando los siguientes comandos:\n",
    "# Para obtener más información sobre el montaje, consulta en: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "show(df.head())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el 91% de la gente deja muy buena nota en el *rating*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verified_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA #3: EXPLORAR EL DATASET\n",
    "\n",
    "### Analizamos los NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels= False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(13,5), color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es un dataset bastante desbalanceado, hay muchas más valoraciones\n",
    "positivas que negativas.\n",
    "\n",
    "**Vamos a añadir una columna con la longitud de cada valoración:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['verified_reviews'].apply(len)\n",
    "df.head()\n",
    "#show(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(bins = 100, kind = 'hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay mucha gente que escribe valoraciones cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['length'] == 2851]['verified_reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['length'] == 1]['verified_reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las valoraciones suelen tener una media de 132 carácteres.\n",
    "* El que más ha escrito han sido 2851 palabras y el que menos 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = df[df['feedback'] == 1]\n",
    "show(positive)\n",
    "negative = df[df['feedback'] == 0]\n",
    "show(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['feedback'], label = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'rating', data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].hist(bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a ver el campo *variation* que nos indica el tipo de producto:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(40,15))\n",
    "sns.barplot(x='variation', y = 'rating', data=df, palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Juntamos todas las valoraciones:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =df['verified_reviews'].tolist()\n",
    "len(sentences)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- Unimos todas las frases separadas por un espacio en blanco:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_one_string= \" \".join(sentences)\n",
    "sentences_as_one_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Visualizamos las palabras y su frecuencia:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(WordCloud().generate(sentences_as_one_string))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las palabras *Alexa* y *love* sobresalen del resto.\n",
    "\n",
    "**Ahora vamos a hacer lo mismo pero para las palbras negativas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_list = negative['verified_reviews'].tolist()\n",
    "negative_as_one_string = \" \".join(negative_list)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(WordCloud().generate(negative_as_one_string))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA #4: LLEVAR A CABO LA LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas que no necesitamos\n",
    "df = df.drop(['date', 'rating', 'length'], axis = 1)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a pasar el tipo de producto a dato numérico para trabjar mejor con\n",
    "estos(pasamos a variables dummie):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_dummies = pd.get_dummies(df['variation'], drop_first=True)\n",
    "variation_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Borramos la columna *variation* ya que ya no nos hace falta y agregamos la\n",
    "columna dummie creada arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['variation'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, variation_dummies], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #5:Eliminamos los sígnos de puntuación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Frase de prueba:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Hi client! Im so happy to be here with you!!!\"\n",
    "test_punc_remove = [char for char in Test if char not in string.punctuation]\n",
    "test_punc_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_punc_remove_join = ''.join(test_punc_remove)\n",
    "test_punc_remove_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Nuestro dataset:**\n",
    "\n",
    "# TAREA 6: ENTENDER COMO LIAR LAS STOPWORDS\n",
    "\n",
    "**Eliminamos las llamadas palabras de parada:** Son palabras como 'yo', 'tu',\n",
    "'y' que no aportan información como los signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como depende del idioma hacemos para el inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "show(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_punc_remove_join_clean = [word for word in test_punc_remove_join.split() if word.lower() not in stopwords.words('english')]\n",
    "test_punc_remove_join_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA 7: ENTENDER EL PROCESO DE TOKENIZACIÓN\n",
    "![alt text](https://drive.google.com/uc?id=10dlXiQMdvzLJwNqhnvylAix9_LUQCGQa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_data = ['This is the first document.', 'This is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(sample_data)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "#version tokenizada\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA #8: LLEVAR A CABO LA LIMPIEZA DE DATOS APLICANDO TODO LO QUE HEMOS\n",
    "APRENDIDO!!\n",
    "\n",
    "\n",
    "### Definimos un pipeline para limpiar todos los mensajes y quedarnos con las\n",
    "palabras más relevantes.\n",
    "\n",
    "**1.1 - Forma manual**\n",
    "Creamos una función que elimine los signos de puntuación y eliminamos los\n",
    "stopwords.\n",
    "*(los stopwords son palabras que no nos dan ninguna info relevante como\n",
    "pronombres o conectores)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_cleaning(message):\n",
    "  test_punc_remove = [char for char in message if char not in string.punctuation]\n",
    "  test_punc_remove_join = ''.join(test_punc_remove)\n",
    "  test_punc_remove_join_clean = [word for word in test_punc_remove_join.split() if word.lower() not in stopwords.words('english')]\n",
    "  return test_punc_remove_join_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df['verified_reviews'].apply(message_cleaning)\n",
    "\n",
    "#Comprarmos como nos ha quedado a como era:\n",
    "print(df_clean[3])\n",
    "print(df['verified_reviews'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 - Aplicando una función ya prefabricada en una librería:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=message_cleaning)\n",
    "reviews_countvectorizer = vectorizer.fit_transform(df['verified_reviews'])\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Tokenizamos el vector:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_countvectorizer.toarray())\n",
    "print(reviews_countvectorizer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Eliminamos y pegamos la nueva columna tokenizada:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['verified_reviews'], axis=1, inplace=True)\n",
    "\n",
    "#Para añadir la nueva columna que está en array hay que pasarla a dataframe primero\n",
    "reviews = pd.DataFrame(reviews_countvectorizer.toarray())\n",
    "\n",
    "df = pd.concat([df, reviews], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*4 - Separamos nuestros datos de entrada de nuestra variable a predecir:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['feedback'], axis = 1)\n",
    "y = df['feedback']\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA #9: ENTENDER LA TEORÍA Y LA INTUICIÓN DETRÁS DE NAÏVE BAYES\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1AyMGr5pZWQ3frRD_84NAM1k765dZNm0S)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1lLQR8v2Iu8-uXzC7sgNdzVAXjtAkvh2z)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1KuifCAosRqw0VG0d9itxul4Yjm1LFfHX)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1XOfRNtgEn6NQcDu-kats3rVMh75hrzZW)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1JYeNmZFOMDWzzW82ARq851WVhi38pMmp)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=12kQKrWY3pvAWaLazeLSfUrgiQIisd0qX)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1q3SWsZkfUtxnI6r-uqBZvK8KSnZ_R2qj)\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1swfSHTNQZGOwbPOSc6J775Fe8YDT7A2J)\n",
    "\n",
    "# TAREA #10: ENTRENAR UN MODELO CLASIFICADOR DE NAÏVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_multinomial = MultinomialNB()\n",
    "nb_multinomial.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA #11: VALIDAR LA EFICACIA DEL MODELO ENTRENADO\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1ZKbO9tQH3qh0IjKwgOAJ37ghWzUVcb8O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_predict_train = nb_multinomial.predict(X_train)\n",
    "y_predict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = nb_multinomial.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que nuestro modelo capta muy bien las valoraciones positivas (tenemos un recall de 0.98) mientras que las negativas las detecta bastante mal. Ya que nuestro dataset no está muy bien balanceado.\n",
    "\n",
    "# TAREA #12: RETO FINAL - ENTRENAR Y EVALUAR UN MODELO CLASIFICADOR BASADO EN LA\n",
    "REGRESIÓN LOGÍSTICA\n",
    "\n",
    "# UN TRABAJO EXCELENTE! PUEDES SENTIRTE ORGULLOSO DE TODO LO QUE ACABAS DE\n",
    "APRENDER"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
